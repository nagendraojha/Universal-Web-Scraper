{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b277e6af-84b3-4270-952d-7cce0bb40b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nagen\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: selenium in c:\\users\\nagen\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nagen\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\nagen\\anaconda3\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\nagen\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from selenium) (4.13.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\nagen\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 requests selenium pandas lxml webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbec75c-bfa0-4068-9a34-9505f015f6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586bd38-b65e-4f78-bc04-6b0f0238bf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a75bc-0ec1-4d2f-9d4b-f07ac4391385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7aa9d-c215-4f36-ad09-49ad29db9d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f295022-1614-4f07-a5d1-7de1e471d7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Universal Web Scraper ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the URL to scrape:  https://skytraxratings.com/airlines/british-airways-rating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What elements do you want to scrape? (comma separated)\n",
      "Examples: ratings, reviews, price, title, description, images, links\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your elements:  rating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you need to handle dynamic content? (JavaScript-rendered pages)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Use Selenium? (y/n):  y\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "\n",
    "class UniversalScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.scraped_data = {}\n",
    "        \n",
    "    def get_user_input(self):\n",
    "        \"\"\"Get scraping parameters from user\"\"\"\n",
    "        print(\"=== Universal Web Scraper ===\")\n",
    "        url = input(\"Enter the URL to scrape: \").strip()\n",
    "        \n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "            \n",
    "        print(\"\\nWhat elements do you want to scrape? (comma separated)\")\n",
    "        print(\"Examples: ratings, reviews, price, title, description, images, links\")\n",
    "        elements = input(\"Your elements: \").strip().lower().split(',')\n",
    "        elements = [e.strip() for e in elements]\n",
    "        \n",
    "        print(\"\\nDo you need to handle dynamic content? (JavaScript-rendered pages)\")\n",
    "        use_selenium = input(\"Use Selenium? (y/n): \").strip().lower() == 'y'\n",
    "        \n",
    "        custom_selectors = {}\n",
    "        if input(\"Do you want to specify custom CSS selectors? (y/n): \").strip().lower() == 'y':\n",
    "            for element in elements:\n",
    "                selector = input(f\"Enter CSS selector for {element} (or press Enter for auto-detection): \").strip()\n",
    "                if selector:\n",
    "                    custom_selectors[element] = selector\n",
    "        \n",
    "        output_file = input(\"Enter output filename (without extension): \").strip() or 'scraped_data'\n",
    "        \n",
    "        return url, elements, use_selenium, custom_selectors, output_file\n",
    "    \n",
    "    def scrape_website(self, url, elements, use_selenium, custom_selectors):\n",
    "        \"\"\"Main scraping function\"\"\"\n",
    "        self.scraped_data['url'] = url\n",
    "        self.scraped_data['domain'] = urlparse(url).netloc\n",
    "        \n",
    "        try:\n",
    "            if use_selenium:\n",
    "                self.scrape_with_selenium(url, elements, custom_selectors)\n",
    "            else:\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                self.extract_elements(soup, elements, custom_selectors)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.scraped_data['error'] = str(e)\n",
    "            \n",
    "        return self.scraped_data\n",
    "    \n",
    "    def scrape_with_selenium(self, url, elements, custom_selectors):\n",
    "        \"\"\"Handle JavaScript-rendered pages with Selenium\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "        \n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            # Wait for page to load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Scroll to bottom to load lazy-loaded content\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Get page source after JavaScript execution\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            self.extract_elements(soup, elements, custom_selectors)\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def extract_elements(self, soup, elements, custom_selectors):\n",
    "        \"\"\"Extract specified elements from the page\"\"\"\n",
    "        for element in elements:\n",
    "            if element in custom_selectors:\n",
    "                # Use user-provided selector\n",
    "                self.scraped_data[element] = self.extract_with_selector(soup, custom_selectors[element])\n",
    "            else:\n",
    "                # Auto-detect based on common patterns\n",
    "                if element == 'ratings':\n",
    "                    self.scraped_data[element] = self.extract_ratings(soup)\n",
    "                elif element == 'reviews':\n",
    "                    self.scraped_data[element] = self.extract_reviews(soup)\n",
    "                elif element == 'price':\n",
    "                    self.scraped_data[element] = self.extract_price(soup)\n",
    "                elif element == 'title':\n",
    "                    self.scraped_data[element] = self.extract_title(soup)\n",
    "                elif element == 'description':\n",
    "                    self.scraped_data[element] = self.extract_description(soup)\n",
    "                elif element == 'images':\n",
    "                    self.scraped_data[element] = self.extract_images(soup)\n",
    "                elif element == 'links':\n",
    "                    self.scraped_data[element] = self.extract_links(soup)\n",
    "                else:\n",
    "                    # Generic element extraction\n",
    "                    self.scraped_data[element] = self.extract_generic_element(soup, element)\n",
    "    \n",
    "    # Specialized extraction methods\n",
    "    def extract_ratings(self, soup):\n",
    "        \"\"\"Auto-detect and extract ratings\"\"\"\n",
    "        ratings = []\n",
    "        \n",
    "        # Check common rating patterns\n",
    "        patterns = [\n",
    "            {'selector': '[itemprop=\"ratingValue\"]', 'attr': 'content'},\n",
    "            {'selector': '.rating', 'attr': 'text'},\n",
    "            {'selector': '.star-rating', 'attr': 'text'},\n",
    "            {'selector': '[class*=\"rating\"]', 'attr': 'text'},\n",
    "            {'selector': 'meta[property=\"og:rating\"]', 'attr': 'content'}\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            elements = soup.select(pattern['selector'])\n",
    "            for el in elements:\n",
    "                if pattern['attr'] == 'content':\n",
    "                    value = el.get('content', '').strip()\n",
    "                else:\n",
    "                    value = el.get_text(strip=True)\n",
    "                \n",
    "                if value and (re.match(r'^\\d+(\\.\\d+)?$', value) or '%' in value or 'star' in value.lower()):\n",
    "                    ratings.append({\n",
    "                        'value': value,\n",
    "                        'element': str(el),\n",
    "                        'selector': pattern['selector']\n",
    "                    })\n",
    "        \n",
    "        return ratings if ratings else \"No ratings found\"\n",
    "    \n",
    "    def extract_reviews(self, soup):\n",
    "        \"\"\"Auto-detect and extract reviews\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        # Check common review patterns\n",
    "        patterns = [\n",
    "            {'selector': '[itemprop=\"review\"]', 'text_selector': '[itemprop=\"reviewBody\"]'},\n",
    "            {'selector': '.review', 'text_selector': '.review-text'},\n",
    "            {'selector': '.testimonial', 'text_selector': '.testimonial-content'},\n",
    "            {'selector': '[class*=\"comment\"]', 'text_selector': ''},\n",
    "            {'selector': '[class*=\"review\"]', 'text_selector': ''}\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            review_containers = soup.select(pattern['selector'])\n",
    "            for container in review_containers:\n",
    "                review = {}\n",
    "                \n",
    "                # Extract review text\n",
    "                if pattern['text_selector']:\n",
    "                    text_el = container.select_one(pattern['text_selector'])\n",
    "                else:\n",
    "                    text_el = container\n",
    "                \n",
    "                if text_el:\n",
    "                    review['text'] = text_el.get_text(' ', strip=True)\n",
    "                \n",
    "                # Extract author if available\n",
    "                author_el = container.select_one('[itemprop=\"author\"], .review-author, .author, .user-name')\n",
    "                if author_el:\n",
    "                    review['author'] = author_el.get_text(strip=True)\n",
    "                \n",
    "                # Extract rating if available\n",
    "                rating_el = container.select_one('[itemprop=\"ratingValue\"], .review-rating, .rating-value')\n",
    "                if rating_el:\n",
    "                    review['rating'] = rating_el.get_text(strip=True)\n",
    "                \n",
    "                # Extract date if available\n",
    "                date_el = container.select_one('[itemprop=\"datePublished\"], .review-date, .date')\n",
    "                if date_el:\n",
    "                    review['date'] = date_el.get_text(strip=True)\n",
    "                \n",
    "                if review:\n",
    "                    reviews.append(review)\n",
    "        \n",
    "        return reviews if reviews else \"No reviews found\"\n",
    "    \n",
    "    # Other specialized extraction methods\n",
    "    def extract_price(self, soup):\n",
    "        \"\"\"Auto-detect and extract price\"\"\"\n",
    "        patterns = [\n",
    "            {'selector': '[itemprop=\"price\"]', 'attr': 'content'},\n",
    "            {'selector': '.price', 'attr': 'text'},\n",
    "            {'selector': '[class*=\"price\"]', 'attr': 'text'},\n",
    "            {'selector': 'meta[property=\"product:price\"]', 'attr': 'content'}\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            elements = soup.select(pattern['selector'])\n",
    "            for el in elements:\n",
    "                if pattern['attr'] == 'content':\n",
    "                    value = el.get('content', '').strip()\n",
    "                else:\n",
    "                    value = el.get_text(strip=True)\n",
    "                \n",
    "                if value and any(c.isdigit() for c in value):\n",
    "                    return {\n",
    "                        'value': value,\n",
    "                        'element': str(el),\n",
    "                        'selector': pattern['selector']\n",
    "                    }\n",
    "        \n",
    "        return \"No price found\"\n",
    "    \n",
    "    def extract_title(self, soup):\n",
    "        \"\"\"Extract page title\"\"\"\n",
    "        title = soup.title.string if soup.title else ''\n",
    "        og_title = soup.find('meta', property='og:title')\n",
    "        if og_title and og_title.get('content'):\n",
    "            return og_title['content']\n",
    "        return title if title else \"No title found\"\n",
    "    \n",
    "    def extract_description(self, soup):\n",
    "        \"\"\"Extract page description\"\"\"\n",
    "        description = soup.find('meta', attrs={'name': 'description'})\n",
    "        if description and description.get('content'):\n",
    "            return description['content']\n",
    "        og_description = soup.find('meta', property='og:description')\n",
    "        if og_description and og_description.get('content'):\n",
    "            return og_description['content']\n",
    "        return \"No description found\"\n",
    "    \n",
    "    def extract_images(self, soup):\n",
    "        \"\"\"Extract all images\"\"\"\n",
    "        images = []\n",
    "        for img in soup.find_all('img'):\n",
    "            src = img.get('src', '')\n",
    "            if src:\n",
    "                images.append({\n",
    "                    'src': urljoin(self.scraped_data['url'], src),\n",
    "                    'alt': img.get('alt', ''),\n",
    "                    'width': img.get('width', ''),\n",
    "                    'height': img.get('height', '')\n",
    "                })\n",
    "        return images if images else \"No images found\"\n",
    "    \n",
    "    def extract_links(self, soup):\n",
    "        \"\"\"Extract all links\"\"\"\n",
    "        links = []\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if href and not href.startswith(('javascript:', 'mailto:', 'tel:')):\n",
    "                links.append({\n",
    "                    'text': a.get_text(strip=True),\n",
    "                    'href': urljoin(self.scraped_data['url'], href)\n",
    "                })\n",
    "        return links if links else \"No links found\"\n",
    "    \n",
    "    # Generic extraction methods\n",
    "    def extract_with_selector(self, soup, selector):\n",
    "        \"\"\"Extract elements using custom CSS selector\"\"\"\n",
    "        elements = soup.select(selector)\n",
    "        if not elements:\n",
    "            return f\"No elements found with selector: {selector}\"\n",
    "        \n",
    "        results = []\n",
    "        for el in elements:\n",
    "            result = {\n",
    "                'text': el.get_text(' ', strip=True),\n",
    "                'html': str(el),\n",
    "                'attributes': dict(el.attrs)\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results[0] if len(results) == 1 else results\n",
    "    \n",
    "    def extract_generic_element(self, soup, element_name):\n",
    "        \"\"\"Attempt to auto-detect generic elements\"\"\"\n",
    "        # Try common class patterns\n",
    "        selectors = [\n",
    "            f'.{element_name}',\n",
    "            f'[class*=\"{element_name}\"]',\n",
    "            f'#{element_name}',\n",
    "            f'[id*=\"{element_name}\"]',\n",
    "            f'[data-{element_name}]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            elements = soup.select(selector)\n",
    "            if elements:\n",
    "                return [el.get_text(' ', strip=True) for el in elements]\n",
    "        \n",
    "        return f\"No {element_name} elements found\"\n",
    "    \n",
    "    def save_to_json(self, data, filename):\n",
    "        \"\"\"Save scraped data to JSON file\"\"\"\n",
    "        with open(f'{filename}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"\\nData successfully saved to {filename}.json\")\n",
    "\n",
    "def main():\n",
    "    scraper = UniversalScraper()\n",
    "    url, elements, use_selenium, custom_selectors, output_file = scraper.get_user_input()\n",
    "    scraped_data = scraper.scrape_website(url, elements, use_selenium, custom_selectors)\n",
    "    scraper.save_to_json(scraped_data, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58de3a98-3d7d-4b4c-b96a-895178097af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://bytexl.app/courses/3zhbr3tk5/dbms-mysql/module/435n4egxv/introduction-of-relational-algebra/topic/435n4mr53/read-now-joins-in-sql', 'domain': 'bytexl.app', 'metadata': {'title': 'ByteXL', 'description': 'ByteXL Application', 'keywords': '', 'og': {}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open('try.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# View the content\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23249c-a921-4e79-b995-9358d7e119cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a85bc-ae22-4523-8fc5-434c410b3407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818dc3b9-4c6c-4024-8ebb-937ddbdb281b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
